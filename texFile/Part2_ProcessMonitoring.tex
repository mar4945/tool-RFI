\lhead{\bfseries MONITORING AND LEARNING}
\chapter{Learning-based anomaly detection for steel-making plant }
\label{Chapter:5}
In this Chapter we propose a FDD method leveraging on class-support One-class Support vector machine (OC-SVM)-based technique; the  model is able to discriminate between normal and anomalous operating conditions indicative of an incoming breakdown in a production context where faults are extremely rare. In fact, the OC-SVM method~\cite{scholkopf2001estimating, acernese2019_ECC} relies on the assumption that the entire training sample of the model belongs to the nominal (safe) operating conditions of the production process. However, when the training data set is affected by a significant subsample  of  non-nominal operating conditions, the OC-SVM exhibits weaknesses. For this reason, we combine the OC-SVM with some preparatory steps aiming to maximize performance. Namely, we adopt a PCA as a feature extraction process, and a Gaussian Mixture Model (GMM) technique \LR{for training set selection} %as a clusterization tool  
ensuring a  sufficiently homogeneous yet informative training data set.\\
% Both techniques essentially rely on the fusion of information received from sensors placed along the production process; we obtained one reliable classification criterion to robustly identify anomalous, normal, and intermediate operating conditions. Note that in the sequel, we will refer to anomaly and outlier interchangeably.
\indent Moreover, we compare the performance with a multivariate statistical methodology recently developed for this particular application~\cite{sarda2021}. 
% the method aims to detect anomalous data revealing  an incipient failure or a degradation of process' performances~\cite{sarda2021} in a steel plant. 
This method incorporates two techniques: %a preliminary fault detection is  carried out based on a straightforward and computationally mild static analysis of the available data. Secondly, a  more complex and computationally intensive method is performed, targeting a precise identification of the anomalies based on the dependencies on time of system's conditions and on the outcomes of the first step. More precisely 
a preliminary anomaly detection technique that uses the reweighted minimum covariance determinant (RMCD) estimator is
%It relies on the nature of anomalies  -  outliers in statistical jargon  -i.e., measurements aside from main features of most of the data,  exhibiting their own pattern or even no pattern at all. In fact, the robust distance fuses the information received from sensors displaced on the production process, hence obtaining one reliable measure and the corresponding trade off, easily implementable and adaptable to all process stages. \\
combined with the hidden Markov model (HMM) theory to obtain a robust FD and diagnosis methodology, able to take into account also the time-varying nature of the data.\\
% . More precisely, the HMM is fitted over the distances of the data points from the robust fit stemming from RMCD estimation; this enables the implementation of a dynamic anomaly detection strategy which takes into account the time-varying nature of the data and infers hidden states in the production process along with their state transition probabilities which model the probability of moving towards degradation stages of the production equipment to the final inferred state, denoting a failure.
%The method we developed is a first attempt to overcome the absence of data on faults events. Robust distances are the key element since outliers can only be properly detected  measuring the distance of data points from a robust fit. To this end, 
\indent Among our main contributions we consider: (i) an automatic FD and diagnosis procedure to detect anomalies in large data sets with strongly unbalanced clusters; (ii) the extension of the standard anomaly detection methods to an OC-SVM-based method that provides additional information about the anomaly severity; (iii) the comparison of the OC-SVM-based method with a validated HMM-based method, considering different training sets with different associated operating conditions, in order to highlight the advantages and drawbacks of both methodologies.
The method is tested on data  generated from the rolling mills of the steel production plant Pittini's group, located in the South of Italy.

% We applied both methods (OC-SVM-based and HMM-based) to the available process data of an hot rolling mill line placed in a steel making plant. We considered different training sets with different associated operating conditions, in order to highlight the advantages and drawbacks of both methodologies.

%Frequency domain analysis is most frequently adopted in anomaly detection of rotating machines since the direct correspondences between specific faults and their characteristic frequencies is a priori known (Randall, 2011). This motivates the frequency-domain procedure to identify anomalies behaviour in process parameters, at least those anomalies related to rotating mechanical parts.  Notice that standard spectral analysis is not particularly effective in this case, since the amplitude of the impulsive type signals associated to defects of the bearings is typically much smaller than that of the signal measured in the absence of fault. Thus, it becomes necessary to employ specific frequency domain techniques that enable to discriminate the contributions associated to faults from the characteristic frequencies of the system in the spectra of the vibration signal. For this purpose, one can resort to the envelope analysis, a well-established technique, see Randall and Antoni (XXXREFFF), widely employed in the diagnosis of rotating machines. This analysis requires first that the data be appropriately filtered to emphasize the fault with respect to other components, so as to facilitate the fault detection. The design of such filter is here based on the analysis of the spectral kurtosis (XXXREF).  Since this procedure is computationally much more intensive than the previous one based on multivariate analysis, it provides a complementary tool, which can be employed when the former suggests the possibility of a fault, with the aim of providing further evidence and details on the fault.

%\LR{\AAC{MOVE TO THE INTRODUCTION???} In the context of supervised learning SVM is an established model for data classification and regression [REFXX]. The SVM model generates nonlinear decision boundaries between classes finding linear boundaries in transformed higher dimensional feature space obtained with kernels. The criteria is to choose boundaries such that the margin between data belonging to different classes is maximum while ensuring a good classification.}
\section{Proposed fault detection and diagnosis procedure}\label{sec:methodology}
\LR{In this section it is reported the description of the novel OC-SVM based method for condition monitoring.The method is general and can be applied in different manufacturing contexts in which fault data are rare or missing. First we describe the feature transformation process using PCA giving a brief description of this technique that is also useful for a better  comprehension of the successive steps. Then we introduce GMM salient features and how it can be conveniently use to split data-set into test and training set for OC-SVM. Finally we describe OC-SVM, its ability to discriminate between nominal and anomalous operating conditions given a proper choice of charateristic parameters and its fitting to condition monitoring.}
%In this section we describe a condition monitoring procedure to automatically detect anomalies. The procedure is broad and may be applied to several manufacturing context; in the subsequent section we show how we successfully applied it to a  hot rolling mill lines of steel making plants. We show how the PCA can be cast in the feature extraction process, and how the GMM technique can be used as a tool ensuring a sufficiently homogeneous yet informative training data set. We further introduce the optimization problem representing the OC-SVM method and its ability in \LR{between nominal and anomalous operating conditions }%identifying clusters
%in unlabeled data sets.

\textbf{Notation.} We denote by $\mathbb{R}$ and $\mathbb{N_{+}}$ the sets of real and positive natural numbers, respectively. Uppercase bold variables denote matrices, lowercase bold ones represent vectors, assumed to be column, i.e., $\mathbf{x}=(\cdot,\dots,\cdot)$ is a column vector.

\subsection{Features transformation process}
\LR{Dimensionality reduction is a fundamental step in many machine learning algorithms. In fact high dimensional data sets often include non relevant attributes that might not affect the output of the prediction. PCA is a useful statistical tool that provides a data driven coordinate system to represent high dimensional correlated data in low dimensional space of uncorrelated dimensions, preserving most of the variance of the original data.  This makes data handier, and significantly reduces the computational load associated with the execution of machine learning algorithms on high dimensional data \cite{brunton2019data}\cite{Reddy}.}
\review{
Let $\mathbf{O(t)}=(\mathbf{o}_1(t),\mathbf{o}_2(t),\dots, \mathbf{o}_n(t))^{\top}$, be a collection of random processes(representing the outcome of the sensors installed into the plant) such that for fixed time $t^*$, the generic $\mathbf{o_i(t^*)}$ is a random variable from normal distribution $\mathcal{O}\sim \mathcal{N}(\mathbf{\mu_i},\mathbf{\sigma_i})$, where $\mathbf{\mu_i}$ is the mean , $\mathbf{\sigma_i}$ is the variance. Assume that each process is sampled with fixed step $T_S$ over a predefined time interval $T$ such that we can construct the  sample $\mathbf{O}=(\mathbf{o}_1,\mathbf{o}_2,\dots, \mathbf{o}_n)^{\top}$  where $\mathbf{o}_{i}=(o_{i1},o_{i2},\dots, o_{ip})\in \mathbb{R}^{p}$,and $p\in \mathbb{N}^+$ is obtained as $\frac{T}{T_S}$.
One may extract from each sample   $\mathbf{O}$ a fixed number $N$ of time-series of length $\omega$ --- eventually allowing some overlapping between the windows --- and compute some parameters (also called indices) over the windows for each sensor.  We refer to the output of this stage as the data set of the parameters, i.e., $\mathbf{Z} = (\mathbf{z}_1,\mathbf{z}_2,\dots, \mathbf{z}_n)^{\top} \in \mathbb{R}^{n \times (Nm)}$, with $m$ the number of extracted parameters.}

Table~\ref{tab: parameters} shows the list of parameters extracted in the case study described in this paper. They include mainly time-domain statistical indices, and time-frequency domain parameters from the signal processing theory. For an overview on the definition of these indices and their properties see~\cite{acernese2020_JQME} and references therein. 
% Note that we limited the extraction some parameters to the vibration signals, because their sampling frequencies are very high w.r.t. other sensors and might provide useful information with these parameters.

 Considering the different nature of the measurement $\mathbf{Z}$ is normalized through min-max normalization and then the PCA is applied to generate the final set of features. Namely, for each $\mathbf{z}_j$, compute $\hat{\mu}_{j}=\frac{1}{N} \sum_{i=1}^{N} z_{i j}$, $j=1,\dots, m$ and the sample covariance matrix $\hat{\mathbf{\Sigma}}$ as:
\begin{equation}
    \label{eqn: covMat}
    \hat{\mathbf{\Sigma}}=\frac{1}{N-1}(\mathbf{Z}-\mathbf{M})^{\top}(\mathbf{Z}-\mathbf{M}),
\end{equation}
where $\mathbf{M} = \mathbb{1}_{N}~(\hat{\mu}_1, \dots, \hat{\mu}_m)$ is the sample mean matrix, and $\mathbb{1}_N$ is the identity $N$-vector. The eigenvectors are called principal components (PCs), and they can be ordered according to the ordered rank of the corresponding eigenvalues of $\hat{\mathbf{\Sigma}}$, from the highest to the lowest. The projection of the original parameters in the $k$-dimensional subspace, comprising of the first $k$ eigenvectors $\mathbf{e}_{1}, \ldots, \mathbf{e}_{k}$, is obtained as follows:
% and are associated with the highest variance direction in the feature space.
\begin{equation}
    \label{eqn: pca}
    \mathbf{X}=(\mathbf{Z}-\mathbf{M}) \left(\mathbf{e}_{1}, \ldots, \mathbf{e}_{k}\right).
\end{equation}
With a slight abuse of notation, in the sequel we refer to $n$ as the new number of samples after having generated the features and with $k$ the number of the effective features. 

% \AAC{\begin{rem}
% It is worth to note that (ADD THAT WE CAN USE PCA AND FEATURE EXTRACTION SEPARATELY AND THAT WE COMBINED BOTH PROCEDURES IN ORDER TO COMPRESS AS MUCH INFORMATION AS POSSIBLE FROM THE BIG INITIAL DATASET). LINK THESE STEPS WITH THE FLOW CHART.
% \end{rem}
% }

\subsection{Gaussian mixture model}
Once a data set $\mathbf{X}$ is constructed, one can use it to train a classifier, i.e. a function from the feature space to a finite set of classes, namely $f : \mathbb{R}^k  \mathbb{N}$. We here consider an unsupervised learning setting, i.e., there is no knowledge on the true output labels associated to $\mathbf{X}$. Furthermore, as we stated in section~\ref{sec:prel}, the fault is a very rare event, thus constraining the applicability of unsupervised learning methods to anomaly detection algorithms. On the other hand, we should rely only on data that are dense in the projected feature space because close data are more likely to produce the same output. In light of the above discussion, and given that it is expected that most of data are acquired in nominal working conditions, we here propose a GMM to create an homogeneous training set.

Strictly speaking, a GMM gives a parametric model of the underlying probability density function based on the assumption that data can be obtained from a combination of a set of $K$ $k$-variate gaussian processes $\mathcal{N}_{\{i\}}(\mathbf{\mu}_i,\mathbf{\Sigma}_i)$, with $\mathbf{\mu}_i \in \mathbb{R}^k$, $\mathbf{\Sigma}_i \in \mathbb{R}^{k \times k}$, $i = 1, \dots , K$.
Thus, a GMM can be represented as the following weighted sum:
\begin{equation}
    \label{eqn: gmm}
    p(x | \Theta)=\sum_{i=1}^{K} w_i\mathcal{N}_i(\mathbf{\mu}_i,\mathbf{\Sigma}_i).
\end{equation}
where $\Theta := \{w_i, \mathbf{\mu}_i, \mathbf{\Sigma}_i\},\; i=1,\dots, K$ is the set of all parameters representing the model.\\
\indent Given the observed data set $\mathbf{X}$, the GMM objective is to estimate $\Theta$ such that one can classify each data point $\mathbf{x}_i$ as a sample of the $i$-th cluster. This operation is usually performed through iterative algorithms, e.g., the expectation maximization algorithm or the maximum a posteriori estimation. We avoid the technical description of these iterative methods because it is out of the scope of this paper, but we point to~\cite{brunton2019data} for deeper details. 

\subsection{One class-support vector machine}
The OC-SVM technique is an adaptation of the SVM model to the context of anomaly detection and unsupervised learning \cite{scholkopf2001estimating}. In particular, the whole sample of training set is assumed to belong to a single class --- representing the nominal operating conditions in our case study --- and the aim is to train a model able to detect both normal and anomalous instances. In other words, OC-SVM computes a binary function that presumably captures regions in the input space where probability density lives (its support). To this aim, training data are projected into the feature space, corresponding to the kernel, and then the data points are separated from the origin with an hyperplane while maximizing the margins. This results in solving the following primal optimization problem:
\begin{equation}\label{eqn:primalProb}
\begin{split}
\min_{\mathbf{w}, \xi_{i}, \rho~} &\frac{1}{2}{\mathbf{w}} ^{2}+\frac{1}{\nu n} \sum_{i=1}^{n} \xi_{i}-\rho \\
\text { s.t. } &\langle \mathbf{w}, \Phi(\mathbf{x}_{i})\rangle \geq \rho-\xi_{i} \quad\quad ~i=1, \ldots, n \\
&~\xi_{i} \geq 0 \quad \quad \quad \quad \quad \quad \quad ~~i=1, \ldots, n,
\end{split}
\end{equation}
where $\mathbf{x_i}$ is the training data, $\Phi(\mathbf{x}_i)$ is a nonlinear function projecting training data into feature space, i.e., the kernel function, and $\langle \cdot, \cdot \rangle$ is the inner product operation. Furthermore, $\xi_i$ are the slack variables allowing some data points to lie within the margin, the constant $\nu>0$ determines the trade-off between maximizing the margin and the number of training data points within that margin, and the parameter $\rho$ is the bias.

\begin{figure}
    \centering
	\vspace{0.1cm}
	\input{figure/Chapter5/Figures_LR/figureOC}
	\caption{Feature space of an OC-SVM classifier.}
    \label{fig:oc-svmfig}
\end{figure}

The equation of the hyperplane separating data from the origin into the projected feature space (see Fig.~\ref{fig:oc-svmfig}) is given by $\mathbf{w} \cdot\Phi(\mathbf{x}_i )-\rho=0$, $\forall i$, and the corresponding decision function that estimates the output label associated to $\mathbf{x}_i$, $i = 1, \dots, n$, can be written as:

\begin{equation}\label{eqn:decisor}
    f(\mathbf{x}) := sign(\mathbf{w} \cdot \Phi(\mathbf{x}_i)-\rho).
\end{equation}

Since nonzero slack variables $\xi_i$ are penalized in the objective function, if $\mathbf{w}$ and $\rho$ solve this problem, then the decision function will be positive for most examples $\mathbf{x_i}$ contained in the training set. When this minimization problem is solved using the Lagrange multipliers the decision function rule for a data point $\mathbf{x}$ becomes:
\begin{equation}\label{eqn:decisorLagrange}
f(\mathbf{x}) := sign\biggr(\sum_{i=1}^n \alpha_i K(\mathbf{x},\mathbf{x}_i)-\rho\biggl),
\end{equation}
where $K(\mathbf{x},\mathbf{x}_i)=\Phi(\mathbf{x})^{\top} \Phi(\mathbf{x}_i)$ is the kernel function and $\alpha_i \in \mathbb{R}_{+}$ are the Lagrange multipliers. The kernel must be carefully chosen because it strongly affects the algorithm's performance~\cite{muller2018introduction}. In this work, a gaussian radial basis function is used, namely:
\begin{equation*}
    K(\mathbf{x},\mathbf{x'})= c \cdot e^{{\mathbf{x}-\mathbf{x}'}},
\end{equation*}
where $c$ is the kernel parameter.
\section{Proposed method}
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=17cm]{figure/Chapter5/Figures_LR/flow_chart.png}
    \caption{OC-SVM-based method schematic.}\vspace{-0.2cm}
    \label{fig: flow}
\end{figure*}
 In the literature, the OC-SVM method is often used as a standalone FD technique. When dealing with real data sets it may be difficult to define nominal instances from the whole data set. Thus, we combine the OC-SVM with the GMM to select a reliable training set. Particularly, given $p(x \mid \Theta)$ from the GMM referred to the whole data set, we select the training set of parameters as $\mathbf{Z}_{\{T S\}}:=\left\{\mathbf{z}_j\right\}$, $\forall j \in \boldsymbol{\Gamma}$, where $\boldsymbol{\Gamma}:=\left\{i \mid p\left(\mathbf{x}_{\mathbf{i}} \mid \Theta\right)>\gamma\right\}, \mathbf{x}_{\mathbf{i}} \in \mathbf{X}, i=1, \ldots, n$, and $\gamma: \mathbb{R} \rightarrow[0,1]$ is a user-defined threshold determining the intersection level of the distribution, respectively.

Then, we apply the PCA to the new set $\mathbf{Z}_{\{T S\}}$, and the obtained training set $\mathbf{X}_{\{T S\}}$ is used as input to the OCSVM. Higher values of $\gamma$ make the model more sensitive to the variation in the data set $\mathbf{X}$, thus suggesting the possibility of using the same model for condition monitoring purposes.

In the considered case study, the proposed methodology is extended to the case of three-class clustering through the following approach. Given the OC-SVM score defined as
$$
g(\mathbf{x})=\sum_{i=1}^n \alpha_i K\left(\mathbf{x}, \mathbf{x}_i\right)-\rho,
$$
consider the euclidean distance $d$ between $\mathbf{X}$ and $\max (g)$. Then, we define two thresholds determining whether a data point is classified as warning or anomalous, as $\eta_1=$ $3 / 2 \mathrm{Med}(d)$, and $\eta_2=3 \operatorname{Med}(d)$, respectively, where $\operatorname{Med}(\cdot)$ is the median value.

The complete OC-SVM-based fault classification methodology is graphically represented in Fig. \ref{fig: flow}.
Case study: fault detection in a steel industry} \label{sec:simulations}
In this section, we briefly present a multivariate statistical-based procedure to automatically process field data and consequently classify the production process in anomalous vs normal conditions. Afterward, we compare such a method with the proposed OC-SVM-based procedure and we discuss the results.

\subsection{Multivariate statistical method}
This method has been recently proposed in~\cite{sarda2021}, and it is based on three main steps. 
The first preliminary step is the computation of the profile-wise centering to avoid the effects of heterogeneous data generated by distinct profiles. To this aim, a regression model between the profiles and the available $p$-dimensional measurements is identified. Then, the corresponding residuals between the predicted values and the true ones are normalized through a Huber mean subtraction.

The second step concerns the robust fitting of multivariate location vector ${\mathbf{\mu}}$ and scatter matrix ${\mathbf{\Sigma}}$ over the $p$-dimensional residuals using the RMCD estimator. Indeed, a traditional approach to detecting outliers from a data set is based on the Mahalanobis distance (MD) evaluated on the sample mean $\hat{\mathbf{\mu}}$ and the sample covariance matrix $\hat{\mathbf{\Sigma}}$ of the observed data $\mathbf{O}$. A sample $\mathbf{o}$ is considered anomalous if $d(\mathbf{o};\hat{\mathbf{\mu}},\hat{\mathbf{\Sigma}})$ is larger than some fixed threshold; consequently, the motivation behind the use of the RMCD estimator is the exploration of methods to robustly estimate the parameters of the $p$-variate Normal distribution, i.e., $\hat{\mathbf{\mu}}_{RMCD}$ and $\hat{\mathbf{\Sigma}}_{RMCD}$, and to compute a reliable feature estimates, i.e., the MDs.
The last step is the training of an HMM that, starting from the measurements, estimates the hidden states representing the working conditions of the machine, along with their transition probabilities.
\subsection{Pittini data set}
We collected production data from October 2019 to November 2020, for a total number of produced billets equal to 8612. We focused on a specific Pittini product (referred to as $\varnothing 8$ ) and monitored only the very last cage, namely the 18th one. The choice of limiting the analysis to this profile is two-fold: i) $\varnothing 8$ is the most produced one, and ii) it is the profile that majorly stresses the motors in terms of high RPM. Moreover, the last cage is the most vulnerable to failures: it continuously works near the threshold of its nominal operating conditions, applying to bars the highest rate of deformation and stretch of the rolling mill line.

When limited to $\varnothing 8$ and cage 18 th, the resulting number of worked billets reduced to 1305. To validate the performance we used the maintenance interventions (MIs) and the scheduled visual inspections (VIs) performed by the maintenance technicians in the period under investigation. The scheduled maintenance consists of VIs of easily accessible components and subsequent replacement of downgraded parts, and half-yearly MIs that deal with the replacement of core parts of the plant that are difficult to access, e.g., motors, gearboxes, bearings, etc. Table II summarizes these interventions; in the case of VIs, we can discriminate 3 increasing severity levels, namely green, orange, and red. In addition to this information, a real breakdown of the reducer of cage 18 occurred in October $31,2019$.
\subsection{Results and discussion}

\begin{figure}[h!]
    \begin{subfigure}{.5\textwidth}    
    \centering
    \input{figure/Chapter5/Figures_AC/trainSmall}
    
    \end{subfigure}
        \begin{subfigure}{.5\textwidth}
    \centering
    \input{figure/Chapter5/Figures_AC/testSmall}
     
    \end{subfigure}
\caption{(a)Train set with 97 samples ($\gamma=0.7$),(b)Performance of the model trained with 97 samples ($\gamma=0.7$)}
    \label{fig:Trai-testsmall}
\end{figure}
We here present the performance of the proposed methodology with respect to Table \ref{tab: maintenance}, along with a comparison with the HMM-based method. In particular, by changing the threshold level $\gamma$ given by the GMM, we considered different data sets. As reported in the previous section, the less is the number of data during the training phase, the more homogeneous should be the corresponding data set. Fig.\ref{fig:Trai-testsmall}-(a) shows the comparison when we have only very few data to train the model, i.e., only 97 training observations corresponding to $\gamma=0.7$. From the figure we can firstly observe that the training set is composed of the samples
that are the most distant from the maintenance interventions given in Table \ref{tab: maintenance}, empirically proving the capability of the GMM to discriminate the healthier data. The validation of the trained classifiers is shown in Fig. \ref{fig:Trai-testsmall}-(b), where we can appreciate the effectiveness of the proposed OC-SVM method. Indeed, even if we give very few samples to the OC-SVM classifier during the training, it is able to correctly identify the boundaries for the warning values (in yellow) and alert ones (in red). By contrast, the HMM-based method is not able to learn a model that properly represents the condition of the machine, resulting in an extremely conservative model in the validation phase, as we can observe through the high number of warning and alarm points erroneously classified in Fig. 5. As per the comparison with Table \ref{tab: maintenance}, we can see that yellow and red samples are correctly classified because they coincide with the most severe inspected conditions, i.e., the breakdown event in October 2019, and the replacemets of the reducers in August 2020 and at the end of November 2020.
\begin{table}[h]
	\centering
    \caption{Available spot-conditions of the mill line}
	\begin{tabular}{c c l c}
    \toprule
	Date  & Type & Description & Severity level\\
	\midrule
	\cellcolor[HTML]{EFEFEF}13/11/2019  & \cellcolor[HTML]{EFEFEF}VI &  Motor\cellcolor[HTML]{EFEFEF} & \cellcolor[HTML]{EFEFEF} \textcolor{orange}{Orange}  \\
	\cellcolor[HTML]{EFEFEF}& \cellcolor[HTML]{EFEFEF}& \cellcolor[HTML]{EFEFEF}Reducer & \cellcolor[HTML]{EFEFEF} \textcolor{orange}{Orange}  \\
% 	\hline
	01/01/2020  & MI & Replacement of the reducer& ---\\ 
	& & and the motor &\\
% 	\hline
	\cellcolor[HTML]{EFEFEF}09/07/2020  &\cellcolor[HTML]{EFEFEF} VI &\cellcolor[HTML]{EFEFEF} Motor &\cellcolor[HTML]{EFEFEF} \textcolor{green!60!black}{Green} \\
	\cellcolor[HTML]{EFEFEF}&\cellcolor[HTML]{EFEFEF} &\cellcolor[HTML]{EFEFEF} Reducer &\cellcolor[HTML]{EFEFEF} \textcolor{red}{Red} \\
% 	\hline
	-\,/08/2020 & MI & Replacement of the reducer& ---\\ 
	& & and the motor &\\
% 	\hline
	\cellcolor[HTML]{EFEFEF}28/10/2020  &\cellcolor[HTML]{EFEFEF} VI &\cellcolor[HTML]{EFEFEF} Motor &\cellcolor[HTML]{EFEFEF} \textcolor{red}{Red} \\
	\cellcolor[HTML]{EFEFEF}&\cellcolor[HTML]{EFEFEF} &\cellcolor[HTML]{EFEFEF} Reducer &\cellcolor[HTML]{EFEFEF} \textcolor{orange}{Orange} \\
% 	\hline
	24/11/2020 & MI & Replacement reducer & ---\\ 
	& &  &\\
	\toprule
\end{tabular}
\label{tab: maintenance}
\end{table}	
Fig. \ref{fig:Trai-testLArge} shows the same analysis performed on a richer training data set, obtained by choosing $\gamma=0.3$ in the GMM, with a resulting training data set of 744 samples. As we can see from Fig. \ref{fig:Trai-testLArge}-(a) and Fig. \ref{fig:Trai-testLArge}-(b), the performance of the HMM-based model significantly increases when a significant sample is provided for the training, comprising of normal and near-warning working conditions. On the other hand, OC-SVM lightly decreases its performance by exhibiting very few warning conditions in August and September, even if it is still able to correctly classify most of the data.
\begin{figure}[h!]
    \begin{subfigure}{.5\textwidth}    
    \centering
    \input{figure/Chapter5/Figures_AC/trainLarge}
    
    \end{subfigure}
        \begin{subfigure}{.5\textwidth}
    \centering
    \input{figure/Chapter5/Figures_AC/testLarge}
     
    \end{subfigure}
\caption{(a)Train set with 744 samples ($\gamma=0.3$),(b)Performance of the model trained with 744 samples ($\gamma=0.3$)}
    \label{fig:Trai-testLArge}
\end{figure}
From this analysis, it is clear that the OC-SVM-based method can be successfully applied to the FD in steel industries. The method exhibits good performance in revealing the actual health status of the machine, and outperforms the HMM-based approach whether a very few data for the training is given. Moreover, in Fig. \ref{fig: degrad} we can see the estimated labels corresponding to the samples around the breakdown. From the figure, one can appreciate a degradation trend starting from the day before the fault. This behaviour suggests that the identified features strongly represent the health status of the working machinery, thus allowing potential extensions of the proposed approach also to the prediction of the remaining useful life. However, we are aware that predictive maintenance models rely on a huge amount of run-to-failure field data, thus we preserve the prediction challenge to further investigations.
\begin{figure}[h]
	\centering
	\vspace{0.1cm}
	\input{figure/Chapter5/Figures_AC/degradation}
	\vspace{-0.2cm}
	\caption{Prediction ability of the proposed method.}
	\label{fig: degrad}
\end{figure}





\chapter{Signal-based and learning-based integration}


\label{Chapter:6}

In this Chapter, we propose a diagnostic scheme for Process monitoring of mechanical components. The proposed scheme combines anomaly detection algorithms, envelope analysis of vibration data, and eventually additional qualitative information on machine functioning. The combination of all the fault indicators is obtained by leveraging a fuzzy inference system. The proposed scheme is experimentally validated on a steel-making plant with real process data, making use of heuristic information such as monitoring reports of machine health status.
\section{Monitoring of industrial components}\label{sec:methodologies}
In this section we describe in detail the rationale of our method.


Building upon previous researches on AD methods \cite{sarda2021,russoOCSVM,9589440}, in this paper we adopt two AD procedures, and EA of vibration signals  (see Figure \ref{fig:rationale}). 
%
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figure/Chapter6/Figure_MM/fuzzy_main_rationale_2.pdf}
    \caption{Rationale of the proposed methodology.}
    \label{fig:rationale}
\end{figure*}
%
%In the methodology we are here proposing AD is performed by computing features from a batch of process measurements (which may include general process parameters such as temperature, pressure, current, voltage, vibration ) via three methods: 
{Specifically to AD}, two general purpose approaches have been chosen, namely: (i) an hidden Markov model (HMM) based on the reweighted minimum covariance determinant (RMCD) estimator; (ii) {a} one-class support vector machine (OCSVM) approach, with  training set selection based on principal component analysis (PCA)  and gaussian mixtures model (GMM). In contrast, EA is mainly used to process vibration data collected by accelerometers mounted close to rotating components.

The above algorithms rely on different mathematical instruments: the HMM is a statistical method, the OCSVM approach is a machine learning one, and the EA approach is a signal processing tool.  {The selection of the most appropriate and effective method to detect anomalies might be a challenging task when there are no labels}: {thus, a technique that provides one synthetic and reliable output is desirable}. The proposed FIS architecture helps in reaching this objective, as well as including qualitative information to detect anomalies.

The FIS comprises: (i) a fuzzifier unit, that fuzzifies the input data through the definitions of a set of membership functions (MFs); (ii) a knowledge base, that defines a set of IF-THEN rules, i.e.,
IF a set of conditions (antecedent) is satisfied, THEN a set of conditions (consequent) can be inferred; (iii) the inference engine, that computes the rules strengths to infer the knowledge from the knowledge base; (iv) a defuzzifier, which produces a crisp final output.
FIS are mainly classified as Takagi-Sugeno-Kang (TSK) type \cite{6313399} {and} Mamdani-type \cite{mamdami}. 
In the latter, a rule’s consequent part is a fuzzy set, whereas, in
TSK-type, a rule’s consequent part is a polynomial function of the inputs, see \cite{OJHA2019845} for a recent review. 

\begin{remark}

Each method comprises a training phase and an evaluation phase. In the training phase, the parameters of the method are first tuned on a collected dataset. In the evaluation phase, the tuned method is employed by processing incoming plant data.
\end{remark}

\lhead{\textbf{\rightmark}}

%%% There is missing part here only because the OCSVM and HMM have already been introduced in the previous Chapter

\subsection{EA of rotating components}\label{subsec:envelope}

 
EA refers to the  algorithmic steps usually employed {to diagnose faults in rotating components, such as gears and bearings leveraging vibration data.} This topic has been extensively studied in the literature \cite{randall2011vibration}. The underlying idea is to look for known frequencies in vibrations data. This knowledge come{s} from the physics of the component: for instance, typical fault frequencies for bearings are the Ball Pass Inner Race Frequency (BPFI), Ball Pass Outer Race Frequency (BPFO), and Fundamental Train Frequency (FTF), that largely increase their amplitude when a fault on the inner race, outer race and balls of the bearing are damaged, respectively. Those frequencies can be analytically computed based on the bearing geometry.
 
The typical EA approach consists in the following main steps \cite{RANDALL2011485}: (i) if possible, get vibration data $v(t)$ at constant rotating speed of the component under monitoring, where $t$ is the sampled time; (ii) perform a series of filtering operations using auto-regressive models, adaptive noise cancellation, time synchronous averaging or minimum entropy deconvolution, obtaining the signal $r(t)$; (iii) band-pass filter $r(t)$ in a frequency band discovered with the aid of the spectral kurtosis computation; (iv) compute the envelope signal $h(t)$ by the Hilbert-Huang transform; (v) compute the fast Fourier transform $H(f)$ of the envelope $h(t)$.
%
Looking at the magnitude of $H(f)$, it is possible to search for the amplitude of known fault frequencies: if those amplitudes are above a user-selected threshold then a fault is detected. 
%
The training phase of the method consists in selecting the filtering parameters.
% \AAC{again the part on train/test} Also in this approach there is a tuning phase where, e.g. filters parameters are selected based on a set of healthy and (possibly) faulty data, and an evaluation phase where the algorithm is used on new data. 
%
More information are available in \cite{mazzoleni2021EMA}, and a scheme is reported in Figure \ref{fig:ea_rationale}.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{figure/Chapter6/Figure_MM/envelope_rationale.pdf}
    \caption{Rationale of EA method.}
    \label{fig:ea_rationale}
\end{figure}


EA is considered in our combined approach since it is widely used in the condition monitoring of rotating components, that are widespread in industry. 
% Nonetheless, EA is an approach that deals well with a limited set of data types (mostly vibrations).

% \begin{remark}
% \AAC{not necessary?}EA is an effective and simple way to perform fault diagnosis in almost all its aspects: not only we can be able to tell that a fault is present (\i{fault detection}) but also that a fault is present on the rotating component (\i{fault isolation}) and which kind of fault (\i{fault identification}).
% \end{remark}

\begin{remark}  
It is worth to note that if the working speed of the rotating component under monitoring varies over time, it is necessary to normalize vibration data  with respect to the rotation speed. 
\end{remark}
\subsection{Diagnostic fuzzy logic system}\label{subsec:fuzzy}
This subsection describes how a FIS can be used to design a diagnostic system that combines the quantitative information coming from the outputs of the methods in Sections \ref{subsec:hmm}, \ref{subsec:ocsvm}, and \ref{subsec:envelope} with (possibly) additional qualitative inputs. In this context, the output of the FIS can be a number representing an health indication ranging from completely healthy to completely faulty. 
%
% Fuzzy theory has been employed in fault diagnosis of different systems, such as rotating machines~\citep{BERREDJEM2018134}, railway wheels~\citep{Baban2019}, pumps, or gearboxes~\citep{Cerrada2018}. However, fuzzy systems have been mainly used as core parts of monitoring strategies, i.e. to learn the ``fault symptoms $\varrightarrow$ fault'' relation. While the fuzzy rules can be learnt in a supervised way from data, this strongly limits the scalability and interpretability of FL to complex systems. \CDV{Questo non mi è chiaro} \MM{$\to$ stiamo cercando di motivare nuovamente, come già tentato nell'introduzione, che noi definiamo le regole del FIS "a mano", anzichè "impararle dai dati". Non possiamo imparare dai dati perchè non abbiamo dati labellati, cioè non sappiamo con certezza quando c'è fault e quando no. Questo fatto, adesso che ci penso, andrebbe chiarito meglio nel paper. Sarei quasi dell'idea di tagliare via tutto questo paragrafo. Abbiamo già messo dei reference dell'uso di Fuzzy Logic per diagnosi nell'introduzione, qui non servono più. Diciamo solo che proponiamo un FIS come decision-making rule, mettendo le regole a mano.} 
% %
% Hence, we here propose an FL-based approach that instead only plays a \i{decision-making rule}, i.e., the soft computing technique is responsible to make decisions based on the outputs of different fault detection algorithms, along with possible heuristic information. 
%
In the sequel, we limit the description of the FIS to the Mamdani type, {since such models are more intuitive and open to interpretability}.
% It was first introduced as a method to create a control system by synthesizing a set of linguistic control rules obtained from experienced human operators.
% In a Mamdani system, the output of each rule is a fuzzy set. 
% Since Mamdani systems are more intuitive and have easier to understand rule bases, they are well-suited to ES applications where the rules are created from human expert knowledge. 

In the FL theory, propositions in the IF-THEN rules can take real values in $[0,1]$. This concept is called degree of membership. In general, fuzzy inference is a method that maps the values of an input space ${U}$ to values of an output space ${Y}$, based on some set of rules. Let $u \in {U} \subseteq \mathbb{R}$ be an element of the input space ${U}$. A fuzzy set $\mathscr{U}$ is an extension of the classical set, and is defined as the set of ordered pairs
\begin{equation}
    \mathscr{U} := \{u, \mu_{\mathscr{U}}(u) | u \in {U} \},
\end{equation}
where $ \mu_{\mathscr{U}}(u) : \mathbb{R} \varrightarrow [0, 1]$ is called the membership function (MF) of $u$ in $\mathscr{U}$, i.e., the MF maps each $u \in {U}$ to a membership degree of such element in the fuzzy set $\mathscr{U}$. An element $u$ is considered not included in $\mathscr{U}$ if $\mu_{\mathscr{U}}(u) = 0$, totally included if $\mu_{\mathscr{U}}(u) = 1$, and a fuzzy member otherwise. Generally speaking, MFs are designed by experts using personal experience, or extracted from numerical data, and several types of MFs have been successfully employed in the literature.


Once the crisp input $u$ is fuzzified, one has to determine how different fuzzy inputs can be combined together and processed. This is done through the inference engine and the knowledge base.
The inference engine is a natural extension of the Boolean logic to fuzzy numbers. Let 
$$\alpha_{i,j} :=\mu^{[j]}_{\mathscr{U}_{i}}\left(u\right),$$ 
be the fuzzified value of an input $u \in {U}$ in the fuzzy set $\mathscr{U}_i$, where the superscript $[j]$ refers to the $j$-th input.
Then, the Boolean operations $\alpha_{i,1} \wedge \alpha_{j,2}$, $\alpha_{i,1} \vee \alpha_{j,2}$, and $\neg \, \alpha_{i,1}$, can be easily extended to the more general case in which $\alpha_{i,1}, \alpha_{j,2} \in [0,1]$ by substituting such operators with $min(\alpha_{i,1}, \alpha_{j,2})$, $max(\alpha_{i,1}, \alpha_{j,2})$, and $1-\alpha_{i,1}$, where $min(\cdot, \cdot)$, $max(\cdot, \cdot)$, $\wedge, \vee$, and $\neg$ represent the minimum, maximum, and logical AND, OR, NOT operators, respectively. 
%
% The inference system is a natural extension of the Boolean logic to fuzzy numbers. \MM{Let $\alpha_1 := a^{[1]}_{\mathscr{U}_{1}}=\mu^{[1]}_{\mathscr{U}_{1}}\left(u_1\right)$, $\alpha_2 := a^{[2]}_{\mathscr{U}{_2}}=\mu^{[2]}_{\mathscr{U}_{2}} \left(u_2\right)$ be the results of evaluating two inputs $u_1, u_2 \in \bf{U}$ in the MFs $\mu^{[1]}_{\mathscr{U}_1}\left(u_1\right)$ and  $\mu^{[2]}_{\mathscr{U}_2}\left(u_2\right)$ respectively, where the apex $[i]$ denotes a MF for the $i$-th input and $\mathscr{U}_1, \mathscr{U}_2$ are two fuzzy sets.
% The Boolean operations $\alpha_1 \wedge \alpha_2$, $\alpha_1 \vee \alpha_2$, and $\neg \alpha_1$, can be easily extended to the more general case in which $\alpha_1, \alpha_2 \in [0,1]$ by substituting such operators with $min(\alpha_1, \alpha_2)$, $max(\alpha_1, \alpha_2)$, and $1-\alpha_1$,} where $min(\cdot, \cdot)$, $max(\cdot, \cdot)$, $\wedge, \vee$, and $\neg$ represent the minimum, maximum, and logical AND, OR, NOT operators, respectively. 
%

The  knowledge base, composed by IF-THEN rules, represents a set of conditional statements that determine the degree to which each logical expression $b$ is satisfied, and outputs a fuzzified element $\mu_{\mathscr{B}}(b)$ through the implication operation  ($\varrightarrow$), generally performed using the minimum operation --- $min(b, \mu_{\mathscr{B}}(b))$ --- where $\mathscr{B}$ is the fuzzy set related to the implication. If one or more rules are simultaneously activated, then the outputs from all the rules are aggregated (using the maximum operation) and the fuzzy sets that represent the output of each rule are combined into a single output fuzzy set
\begin{equation}
    \mathscr{Y} := \{b, \mu_{\mathscr{B}}(b) | b \in \mathscr{B} \}.
\end{equation}

%
The last step is the defuzzification, that maps $\mathscr{Y}$ into crisp numbers $y \in {Y} \subseteq \mathbb{R}$. The centroid method computes the real value $y$ from the fuzzified output as a weighted mean:
\begin{equation}
    y := \frac{\int_{\mathscr{B}} b \cdot \mu_{\mathscr{B}}(b) \; \dif b}  {\int_{\mathscr{B}} \mu_{\mathscr{B}}(b) \; \dif b}.
\end{equation}


The following example schematizes the proposed approach to fault diagnosis and condition monitoring using a Mamdami-type FIS.
\begin{example}\label{example:FIS_diagnostic}
The proposed rationale is presented in Figure \ref{fig:fuzzy_rationale}. Let $u_1, u_2$ be two inputs and $y$ the output of the FIS, indicating the probability of fault occurrence or the system health state. In our approach, the inputs might be generated from two algorithms, e.g., an AD and an EA. Now, assume that $u_1$ and $u_2$ can be linguistically classified in three fuzzy sets: \{\texttt{red}, \texttt{yellow}, \texttt{green}\}, denoting ``alarm'', ``warning'', {and} ``nominal'' component health states, {respectively}.


The FIS has two rules in the knowledge base. Rule 1 for instance reads as: 
\begin{itemize}
\item IF $u_1$ is \texttt{green} or  $u_2$ is \texttt{red} THEN  $b$ is \texttt{green}.
\end{itemize}
Rule 2 is interpreted similarly. Notice that Rule 2 does not depend on $u_2$. 
Assume now that $u_1=3$, and $u_2=140$.
% (notice that the inputs may have different ranges). 
Those crisp inputs are fuzzified for all the MFs. Then, Rule 1 reads as
%
\begin{equation}
b_1 = 
\begin{cases}
& \mu_{\texttt{red}}\left(max\left(\alpha_{\texttt{green}, 1}, \alpha_{\texttt{red}, 2}\right)  \right), \mbox{ if } \mu_{\texttt{red}}(\cdot) \leq max(\cdot)  \\
& max\left(\alpha_{\texttt{green}, 1}, \alpha_{\texttt{red}, 2}\right), \mbox{\qquad\quad otherwise.} \nonumber
\end{cases}
\end{equation}


% \begin{equation*}
%  min\left(max  \left(  \alpha_{\texttt{green}, 1}, \alpha_{\texttt{red}, 2} \right), \mu_{\mathscr{B}}\left(max\left(\alpha_{\texttt{green}, 1}, \alpha_{\texttt{red}, 2}\right)  \right)  \right)   
% \end{equation*}
 
 
 
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figure/Chapter6/Figure_MM/fuzzy_inference.pdf}
    \caption{Rationale of the proposed FIS for fault diagnosis and condition monitoring.}
    \label{fig:fuzzy_rationale}
\end{figure*}



The MFs of the output are again constituted by the levels  \{\texttt{red}, \texttt{yellow}, \texttt{green}\}, according to the degree of fault occurrence or health condition. The implication, aggregation and defuzzification procedures produce finally a crisp output, that can be reported as an estimate of fault occurrence or machine health status.
\qed
\end{example}



Notice how encoding the inputs in three qualitative classes as \{\texttt{red}, \texttt{yellow}, \texttt{green}\} allows a rationalization and a first comparison  diagnostic indications of the independently employed methods; likewise, a similar encoding of the output allows an easier interpretation for the FIS designer and end users.

The FL methodology described in Example \ref{example:FIS_diagnostic} is used to design a FIS applied to field data coming from a steel production plant, focusing on the diagnosis of a drive reducer. 
%
The main reason for choosing a fuzzy approach lies in the complexity of the process to be monitored, which is nonlinear and composed of many working phases. In addition, it would be unreasonable to expect that each time the same healthy level of the machine arises  the preceding diagnosis algorithms will output exactly the same values. 
The boundaries between different health states are not sharply defined, and therefore the use of a classic true or false logic is inappropriate, and can lead to a large number of false alarms, or, even worse, false negatives. Consequently, MFs and the degrees of membership, rather than a yes or no membership, give the opportunity to define and create a more robust diagnosis tool.
% Furthermore, the FIS provides a way to easily combine the quantitative information coming from such fault detection procedures with additional qualitative data. For instance, in this paper the quantitative inputs are combined with an information representing the visual inspections that the operators occasionally performs on the production line and with external reports, whenever available.

\begin{remark}\label{rem: FL}
The proposed approach allows the inclusion of any qualitative information, such as smell, noise, sound, and others. In this paper, the quantitative inputs are combined with an information representing maintenance reports. 
\end{remark}








% \AAC{In classical logic, a proposition can only take values as true or false. On the other hand, the FL theory introduced the concept that prepositions can take real values from 0 to 1, i.e., the classical theory is a subset of the fuzzy one. This concept is called degrees of membership. It provides a mathematical tool capable of helping with decision taking in an environment with imprecision variables, uncertainty, and incomplete information. In general, fuzzy inference is a method that maps the values of an input space $\mathcal{X}$ to values of an output space $\mathcal{Y}$, based on some set of rules. Let $x \in \mathcal{X} \subseteq \mathbb{R}$ be an element of the input space $\mathcal{X}$. A fuzzy set $\mathcal{Z}$ is an extension of the classical set, and is defined as the set of ordered pairs
% \begin{equation}
%     \mathcal{Z} := \{x, \mu_{\mathcal{Z}}(x) | x \in \mathcal{X} \},
% \end{equation}
% where $ \mu_{\mathcal{Z}}(x) : \mathbb{R} \varrightarrow [0, 1]$ is called the membership function (MF) of $x$ in $\mathcal{Z}$, i.e., the MF maps each $x \in \mathcal{X}$ to a membership degree of such element in the fuzzy set. An element $x$ is considered not included in $\mathcal{Z}$ if $\mu_{\mathcal{Z}}(x) = 0$, totally included if $\mu_{\mathcal{Z}}(x) = 1$, and fuzzy member otherwise. Generally speaking, MFs are desined by experts, from personal experience or extracted from numerical data, and several types of MFs have been successfully employed in the literature. In this work we used Gaussian, triangular, and trapezoidal MFs, whose shapes and boundaries are reported in Fig~\ref{fig:FIS_model} and in Section~\ref{sec: qual_results}, respectively.}

% \AAC{Once the crisp input $x$ is fuzzified, one has to determine how different fuzzy inputs can be combined together and processed. This is done through the rule base system. This block comprises of two subsystems: the logical reasoning and the IF-THEN rules. The former is the natural extension of the Boolean logic to fuzzy numbers. Indeed, given two elements $x_1, x_2 = \{0,1\}$, the Boolean operations $x_1 \wedge x_2$, $x_1 \vee x_2$, and $\neg x_1$, can be easily extended to the more general case in which $x_1, x_2 \in [0,1]$ by substituting such operators with $min(x_1, x_2)$, $max(x_1, x_2)$, and $1-x_1$, where $min(\cdot, \cdot)$, $max(\cdot, \cdot)$, $\wedge, \vee$, and $\neg$ represent the minumum, maximum, and logical AND, OR, NOT operators, respectively. The latter subsystem, i.e., the IF-THEN rules, represents a set of user-defined conditional statements that determines the degree to which each logical expression is satisfied, and outputs a fuzzified element through implication, $\varrightarrow$, that is generally performed using the minimum operation. If one or more rules are activated simultaneously, then the outputs from all the rules are aggregated (using the maximum operation) and the fuzzy sets that represent the output of each rule are combined into a single fuzzy set $\mathcal{\bar{Z}}$. The last step is the defuzzification, that maps $\mathcal{\bar{Z}}$ into crisp numbers $y \in \mathcal{Y} \subseteq \mathbb{R}$. One of the most popular methods used for this process is the centroid one, which computes the real value $y$ from the fuzzified output $\bar{z_i}$ as
% \begin{equation}
%     y := \frac{\int_{\mathcal{\bar{Z}}}z \mu_{\mathcal{\bar{Z}}}(z)dz}{\int_{\mathcal{\bar{Z}}} \mu_{\mathcal{\bar{Z}}}(z)dz}.
% \end{equation}}

% \AAC{The FL methodology described so far has been applied to the case study of a steel production plant to automatically define a decision making process for the health status estimation of the drive reducer. The main reason for choosing a fuzzy approach is the complexity of the process to be monitored. It is nonlinear, and in addition, it would be unreasonable to expect that each time the same real healthy level of the machine arises, the preceding three fault detection algorithms would
% output exactly the same values. The boundaries between different health states not sharply defined, and therefore the use of a classic true or false logic is inappropriate, and can lead to a large number of false alarms, or, even worse, false negatives. MFs and the degree of membership, rather than yes or no membership, give the opportunity to
% define and create a fuzzy rule-based system that can be used as a robust diagnosis tool to monitor the condition of the machine. Consequently, the use of an FL system as a decision making tool is highly justified. Furthermore, the FL provides a way to easily combine the quantitative information coming from such fault detection procedures with additional qualitative data sources generated by the operators. For example, in this paper the quantitative FL inputs are combined with an information representing the visual inspections that the operators occasionally performs on the production line, whenever available.}


% \newpage

\section{Application to a rolling mill process}\label{sec:application}
% \AAC{Should we change one of the figures with a real never used one?}

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figure/Chapter6/Figures_AC/Pittini.jpg}
    \caption{An hot mill rolling line in a steel making plant.}
    \label{fig_Pittini}
\end{figure}
We experimentally tested our  methodology on the hot milling line of a steel making plant placed in the South of Italy (Figure \ref{fig_Pittini}). 
%

Steelmaking is the process of converting scrap metal and/or iron ore into steel. A steel production process consists of a sequence of rotating machines that shape molten steel into bars and billets of various forms and sizes. The primary concern in such industries is monitoring machines' health in view of high throughput, pressure, and temperature in the plant. A breakdown in any of the machines may cause huge damages to both humans and equipment. As a result, identifying faults or signs of deterioration remains an important issue in such industries.

%\subsection{Production process}
\subsection{Experimental setup}\label{subsec:exp_setup}


We studied a steel making plant producing $25$ profiles ($\diameter$, in mm); each profile is processed by a specific combination of the $18$ cages comprising the milling line, at  distinct values of production parameters such as temperature, motor velocity, and pressure applied on the steel bar. In particular, we focused on a single cage, namely the last one of the roll milling line (i.e., cage ~$18$), which is the most vulnerable to failures since it applies the highest rate of deformation and stretch to the bars.

A cage of the rolling mill consists in: (i) an electrical motor, (ii) a gear drive, and (iii) a pinion stand, all connected by couplings see~\citep{sarda2021} for a representation. In particular, we studied the cage's gearbox, made of a motor and a gear reducer. We concentrated on this component since it is the mostly frequently maintained in the cage. 

\tbf{Measurements.} The gearbox sensors are placed in the cage according to the scheme in Figure~\ref{fig_failure_spot}. 
{Specifically, we measure
\begin{itemize}
    \item Three vibration signals measured by three accelerometers placed on the shell of the drive reducer, sampled at $10~\text{kHz}$.
    \item Oil pressure in the drive reducer, acquired at $1~\text{Hz}$.
    \item Two temperature signals are measured by two probes placed on the opposite sides of the reducer, sampled at $1~\text{Hz}$.
    \item Current absorbed by the motor driving the mill, with the sampling frequency of $10~\text{Hz}$.
    \item Revolution speed of the motor, measured in Round Per Minute (RPM) and acquired at $10~\text{Hz}$.
    \item Torque of the motor sampled at $10~\text{Hz}$.
\end{itemize}}


\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figure/Chapter6/Figure_MM/gears.pdf}
    \caption{Gearbox structure, with the positions of the sensors.}
    \label{fig_failure_spot}
\end{figure}
The production line is equipped with a  fully automatic system of production data collection: the system acquires data from the cages and transfers to the server the data corresponding to one registration every $10$ produced billets. In particular, the acquisition starts from $5\,\text{s}$ after the billet is loaded in all the cages of the milling line, and lasts for $15\,\text{s}$ returning a data set with a number of rows equal to the number of samples, and a number of columns equal to the total number of sensors. The highest sampling frequency is $10~\text{kHz}$. Thus, for $15\,\text{s}$, the maximum number of rows is equal to $150,000$. The total number of sensors is $118$. The corresponding impact in terms of memory load is about $100$~MB/billet. The typical interval between two consecutive produced billets data is around $10-15$ minutes.
The total number of produced billets, during the observation of the production system from September 2019 to October 2020, is $4011$. In this period, a failure occurred in October~$2019$.

\tbf{Qualitative inputs}.
As introduced in Remark~\ref{rem: FL}, the FIS can handle both quantitative and qualitative inputs.  In this case study, we exploited one qualitative input that consists of maintenance reports, provided by an external consulting company from time to time during the year.  The outputs of these reports assign one of three levels to the motor and reducer components of the {cage}, as \{\texttt{red}, \texttt{yellow}, \texttt{green}\}, indicating alarm, warning and healthy component conditions, respectively. Note that, in the usage of the FIS algorithm, this input can be provided by the plant operator.

For our study, we assumed that: (i) after a report, the machine conditions do not improve over time; (ii) after a maintenance intervention, the status of the machine is healthy.
The information provided by the maintenance reports is encoded in an input for the FIS as a real number in the range $[1, 3]$, according to Table \ref{tab:coding_qualitative}.

\begin{table}[!ht]
\setlength{\tabcolsep}{12pt}
\caption{Encoding of the maintenance reports indication.}  
\centering
% \settowidth\tymin{{xxxxxxxx}}
% \begin{tabular}{LCCCC}
\begin{tabulary}{1.0\textwidth}{LJJJJ}
\toprule
%
& & \multicolumn{3}{c}{Motor}\\

\cmidrule{3-5}

& & 
\texttt{green}  & 
\texttt{yellow} & 
\texttt{red}  \\ 

 
\multicolumn{1}{c|}{}            & \texttt{green} & 1 & 1.5 & 2\\
 \multicolumn{1}{c|}{Reducer}    & \texttt{yellow} & 1.5 & 2 & 2.5 \\
 \multicolumn{1}{c|}{}           & \texttt{red} & 2 &  2.5 & 3 \\
\bottomrule
% \end{tabular}
\end{tabulary}
\label{tab:coding_qualitative}
\end{table}



\subsection{Tuning of the methods}

\subsubsection{AD and EA methods}

\begin{table}[!ht]
\setlength{\tabcolsep}{12pt}
\caption{Employed measurements for each diagnostic approach.}  
\centering
\begin{tabular}{LCCC}
\toprule
%
% & & Method & \\ 
& \multicolumn{3}{c}{Method}\\
\cmidrule{2-4}
 
Measurement$\qquad$ & $\qquad$HMM$\qquad$ & OCSVM & $\qquad$EA  \\ 
  \midrule
%
  Vibration     & \checkmark (3$^\ast$) & \checkmark (3) & $\qquad$\checkmark (1) \\
  Oil pressure  & \checkmark (1) & \checkmark (1) & \\
  Temperature   & \checkmark (2) & \checkmark (2) & \\
  Motor speed   & & \checkmark & \\
  Motor current & & \checkmark & \\
\multicolumn{4}{l}{\scriptsize{$^\ast (\cdot)$ is the number of sensors used for each kind of measurement.}} \\
\bottomrule
\end{tabular}
\label{tab:maeas_approaches}
\end{table}

We now describe how the general methods proposed in Sections \ref{subsec:hmm}-\ref{subsec:fuzzy} have been tuned to the specific application of monitoring a gearbox in a cage of a rolling mill process. Table \ref{tab:maeas_approaches} reports the measurements employed by each method.

 
Given that the motor current and torque did not exhibit significant results during preliminary testing, we employed the three vibration signals, the oil pressure, and the two temperature signals in the case of the HMM-based AD of Section \ref{subsec:hmm}, for a total of 6 process measurements. A feature vector $\bf{z}\in \bb{R}^{n_z\times 1}$, with $n_z=6$, is computed from each batch of data, as the average of the values from each of the 6 measurements.
The HMM state is set to assume $k=3$ values, which indicate an alarm, warning, or healthy condition.

In the enhanced OCSVM-based methodology, we instead used all the available measurements, given that the method uses a PCA afterward. To allow more stability in the output, we divided the measurements acquisition related to one billet into different windows of size $2\, \text{s}$ with no overlapping between windows.
{For each acquisition, depending on the chosen size and overlap, we have $m$ windows, and for each measurement in the window, we extracted time and frequency domain features, i.e., mean, variance, skewness, kurtosis, {root mean square value, peak value, impulse factor, shape factor, signal to noise ratio, and total harmonic distortion value, see~\citep{sarda2021} for their definitions.} So, considering $N$ acquisitions, the total number of points in the feature space is $N\cdot m$ and the dimension of a single point is given by the number of measurements multiplied by the number of extracted time and frequency domain features.} 
{The resulting output values for a single acquisition are then averaged over the results obtained for each window contained in it, to improve the stability of the health indicator.} 

Differently, in the EA we used only the accelerometer 256 (see Figure \ref{fig_failure_spot}), namely the one placed between the motor and the reducer, as its measurements were observed to  be more stationary and less influenced by external forces.
Prior to the envelope computation, the raw vibration signal $v(t)$ was band-pass filtered at $4\;\text{kHz}$ with a bandwith of $\pm 1\;\text{kHz}$, obtaining the filtered signal $r(t)$. Then, the maximum amplitude of the envelope spectrum $H(f)$, in the $\left[ \text{BPFI}-50\;\text{Hz}, \text{BPFI}+50\;\text{Hz} \right]$, is taken as health indicator. The value of $50\;\text{Hz}$ is set experimentally, since, due to speed variations and noise in the data, the computed fault frequencies do not always exactly align with the theoretical BPFI, computed from the bearing datasheet. This value, as well as the filter parameters, have been tuned by using the faulty data available by the end of October 2019 on the $\diameter 8$ profile\footnote{For the other profiles, the parameters of the EA method were kept the same to those of $\diameter 8$.}.
{For each working speed (that varies with the produced billet profile), we stored the average of the first four values of the indicator. These reference values are used to normalize the subsequent values of the EA indicator, in order to make it comparable across different working speeds.}
In this way, the EA indicator always starts with a value $1$.



\subsubsection{Proposed FIS method}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=15cm]{figure/Chapter6/Figure_MM/rules.pdf}
    \caption{Proposed FIS model for diagnosis with membership functions of inputs and output.}
    \label{fig:FIS_model}
\end{figure*}


Figure~\ref{fig:FIS_model} shows the block diagram representing the developed FIS. It is composed of three quantitative inputs, assumed to be always real numbers and available, and a qualitative one, with the possibility to have ``not available'' in case no maintenance report is present.

The four inputs are fuzzified using three types of MFs: triangular, trapezoidal, and Gaussian. The bounds of each MF have been chosen based on the ranges of the outputs of the ADs and EA methods, by considering their clusterization in \{\texttt{red}, \texttt{yellow}, \texttt{green}\} values,  shown in the next section. Aided by these results, we design a set of 51 rules encoding common-sense rationales for combining the outputs of the quantitative algorithms with the qualitative information provided by the maintenance reports. For instance, consider the rules
\begin{enumerate}
\item IF $u_1$ is \texttt{green} and  $u_2$ is \texttt{green} and $u_3$ is \texttt{green} THEN  $b$ is \texttt{green},
%
\item IF $u_1$ is \texttt{red} and  $u_2$ is \texttt{yellow} and $u_3$ is $\neg \,$ \texttt{red} and $u_4$ is $\neg \,$ \texttt{red} THEN  $b$ is \texttt{yellow},
%
\item  IF $u_1$ is \texttt{red} and  $u_2$ is \texttt{red} THEN  $b$ is \texttt{red},
\end{enumerate}
where $u_1,u_2,u_3,u_4$  denote respectively the outputs of the ADs, EA methods, and maintenance reports, and $y$ is the FIS fuzzy output. Example rules 2. and 3. indicate that at least two \texttt{red} fuzzy values are required to produce a \texttt{red} fuzzy output. Similar rules are applied to cover all possible input combinations, where the qualitative input might be present or not. 


\begin{remark}
Even if the proposed approach requires to processing a batch of measurements that lasts $15\,\text{s}$ to output a result, it functioning is actually online with respect to the process, which requires $10-15$ minutes to produce a billet.
\end{remark}


\section{Results and discussion} \label{sec:simulations}
% \AAC{Given that we mention green, yellow, red in previous section, is it reasonable to move the non FIS results out of this section?}
In this section we report: (i) the results of the AD methods and the EA, singularly applied to the rolling mill process; (ii) their synthesis in one combined output, giving a fault indication as provided by the FIS; (iii) the outcome of the FIS when qualitative data are provided as an additional input to the system.






\subsection{ADs and EA results}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{figure/Chapter6/Fuzzy logic paper/Figures_Plots/Oct2019.pdf} 
    \caption{Results on September - October 2019 data. The bearing failure is well detected by both methods (points that are out of scale are not depicted).}
    \label{fig:oct2019}
\end{figure}  

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{figure/Chapter6/Fuzzy logic paper/Figures_Plots/June_July2020.pdf}
    \caption{Results on June - July 2020 data. An increase in the degradation is observed on the reducer before maintenance.}\vspace{-0.3cm}
    \label{fig:jun2020}
\end{figure}


\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{figure/Chapter6/Fuzzy logic paper/Figures_Plots/Oct2020.pdf}
    \caption{Results on October 2020 data. An increase in the degradation is observed on the reducer before maintenance.}
    \label{fig:oct2020}
\end{figure}

Figure \ref{fig:oct2019}, Figure \ref{fig:jun2020}
and Figure \ref{fig:oct2020} show the results of applying the ADs and EA methods to experimental data. Each point represented in the figures denotes the output of the specific method, computed on a batch of measurement data (one for each single billet) as described in Section \ref{subsec:exp_setup}. 

% (\AAC{No, here there is the combination of train and test data})

{The clusterization of the outputs into \{\texttt{red}, \texttt{yellow}, \texttt{green}\} colors has been performed using ``hard'' thresholds. We skip their formal definitions because it is out of the scope of this paper, but we remind to~\cite{sarda2021} and~\cite{russoOCSVM} for their computations.}

Figure \ref{fig:oct2019} depicts results on production data {gathered} from September 2019 to November 2019. At the beginning of September 2019, the plant underwent a maintenance intervention, {when} the drive reducer of cage 18 was replaced. After this substitution, the EA method does not show warnings, {whereas AD methods indicate a mixed ok/warning alarm in September 2019}. These can be interpreted as early symptoms of the incoming reducer fault, where a bearing broke. We remind that the EA is a ``local'' method processing only vibration data, while the AD {algorithms process} also other variables. Therefore, they are less sensitive to bearing faults but they can {detect} a wider spectrum of process malfunctions. Starting from October 2019, all methods raise warning/alarm indications. The EA approach exhibits a progressive and considerable increment: by November, the bearing fault is detected and identified, and concerning indications could be consistently given also since  October 25th. The AD approaches raise some alarms but these are quite isolated (probably due to an exceed in the gearbox temperature), getting consistent only by the end of October, hence closer to the breakdown event that occurred in October 30th.


Figure \ref{fig:jun2020} reports the results obtained on data recorded in June - July 2020. The no-data period from 19th June to 17th July corresponds to an absence of production. During this rest period, a maintenance report was compiled by an external company in the beginning of July, revealing a warning condition on the reducer. The EA approach did not raise any warning, while the AD methods indicated some warning situations. These warnings can correspond to the output of the maintenance report.
Warning indications by the AD methods persist also after the rest period since no {maintenance} action has been taken {from plant managers} in this regard.  No faults were detected by the company in the June - July 2020 period.



Figure \ref{fig:oct2020} shows the results {of the analysis} on October 2020 data. The initial situation is comparable to that of the end of July 2020 in Figure \ref{fig:jun2020}.  Starting from 8th October, the AD approaches show warning levels until the end of the period, while the EA method does not reveal criticalities. In fact, by the end of October, an external maintenance report has been presented, signaling particular attention to the motor, which is not directly monitored by EA. In the period considered by this figure, no faults were detected by the company.



Clearly, the comparison of the three methods shows that the outputs of  AD and EA analysis give results that generally match; however, sometimes they show differences in alarm signals that could be difficult to interpret. To this end, we propose to employ a FIS to combine their results.


\subsection{Proposed FIS results}\label{sec: qual_results}
This section presents the results of the complete implemented methodology, comprising of the combination of the three quantitative analyses described so far with additional qualitative input data, and the design of an automatic decision-making system leveraging the designed FIS.

The first subplots of Figures~\ref{fig:FIS_results_oct2019}, \ref{fig:FIS_results_june2020}, \ref{fig:FIS_results_oct2020} show the results of the FIS combining the outputs of the ADs and EA algorithms, presenting also the different types of worked profiles.

The FIS has two main benefits. First, it represents a  system that reads three different inputs and makes a decision the human decision-making rules. Second, the FIS smooths the results of the three methods and provides more stable and reliable output trends, that could be easily used to raise alarms or to program the next maintenance interventions.

The bottom subplots of the figures show the FIS results including also the qualitative input, depicted in the second subplot. It is worth noting that, if one compares the outcome of both the FIS analysis (i.e. with and without qualitative information) when heuristic information is not available, the outputs are exactly the same, hence proving that the designed system can work also when these data are not available. However, when qualitative input is {accessible}, the FIS exhibits the most promising results. As one can see in Figure~\ref{fig:FIS_results_oct2019}, the qualitative information during the last week of September strongly stabilizes the estimated health status of the machine. Moreover, a trend representing the degradation of the machine conditions is clearly detectable, as the FIS shows increasing values of fault probabilities or degradation till the end of October when the fault occurred. 

The stabilizing behavior of the FIS is confirmed also in the other two figures, where the FIS is able to recognize false alarms and to scale them into nominal ranges (see, e.g., the points above $0.75$ in Figure~\ref{fig:FIS_results_june2020}). Furthermore, taking into account the near-alarm qualitative data in the last week of October 2020, the relative fault probability in Figure~\ref{fig:FIS_results_oct2020} is increased, even if the three methods separately do not raise particular warnings.



\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{figure/Chapter6/Figures_AC/FIS_results_Oct2019}
    \caption{FIS results during September - October 2019. Different worked billet profiles are represented by different colors.}
    \label{fig:FIS_results_oct2019}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{figure/Chapter6/Figures_AC/FIS_results_June_July2020}
    \caption{FIS results during June - July 2020.}
    \label{fig:FIS_results_june2020}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{figure/Chapter6/Figures_AC/FIS_results_Oct2020}
    \caption{FIS results during October 2020.}
    \label{fig:FIS_results_oct2020}
\end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\section{Conclusions} \label{sec:conclusions}

In this paper, we illustrated a  novel diagnostic scheme for fault diagnosis and condition monitoring in mechanical components.  Motivated by the increasing interest in production data exploitation to improve diagnoses of incoming failures, we also recognized the necessity to develop a methodology able to combine several diagnostic algorithms as well as qualitative information on machine functioning. We developed an ES scheme making use of fuzzy logic. The FIS is able to synthesize, i.e. combine, the outputs of several diagnostic methodologies as well as qualitative information. 

The approach has been proposed by adopting two AD methods, one based on statistical analysis (namely, HMM) and the other on machine learning techniques (i.e., the OC-SVM) as well as classical EA on vibration data of the mechanical component under test.  We explicitly remark that the proposed analytical methods are some of the available AD methods and they could be substituted or integrated with any other method suitable to detect machine malfunctioning. 

Finally, we experimentally validated the proposed methodology on a steel-making plant with process data gathered from the operating process, making use of heuristic information such as monitoring reports performed on the plant. 

The validation analysis shows the benefits of the proposed approach both in synthesizing the machine health status information coming from the AD methods and EA and in integrating the heuristic information. 
To the best of our knowledge, this is the first attempt to include qualitative information into an all-encompassing anomaly detection scheme, hence it deserves further insights.  

Perspective studies should expand the proposed method looking for other synthesizing rules and performing more advanced studies on suitable general-purpose algorithms to be fed to the FIS, as well as focusing on making its rule base more interpretable.

