\addvspace {10\p@ }
\contentsline {figure}{\numberline {1}{\ignorespaces Relevant topics to the Contributions.}}{xvi}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Framework Autonomous System.}}{5}{}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Snapshot extracted from the virtual scenario. A dashed circle is used to indicated the drone position.}}{18}{}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces An illustrative example of the proposed vision-based path following algorithm works. The red point $\mathbf {d}$ represents the drone position, while the orange point $\mathbf {w}$ depicts the~\gls {VTP}.}}{19}{}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Control system architecture. From left to right: the image processing, path planner, controller, and drone blocks.}}{20}{}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces State machine implemented. State 1: \textit {Take-off}. State 2: \textit {Following}. State 3: \textit {End-Marker}. State 4: \textit {Landing}.}}{20}{}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Original frame (upper left), converted and binarized frame (upper right), and eroded frame (lower).}}{21}{}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Arc mask. The drone position (red), the previous~\gls {VTP} (green), and the pre-established path to follow (purple) are reported.}}{23}{}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Frame after the application of the Arc mask (left). Extracted pixels belonging to the path (right).}}{23}{}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Original (left) and eroded frames (right) of the End-Marker are reported.}}{24}{}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces Trajectory plots. From left to right: the desired and the drone paths for various values of $\alpha $ are represented. The mission time $T_s$ and a comparison between the considered $\alpha $ values are also reported.}}{26}{}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces Velocity plot.}}{27}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Flow Diagram of the proposed method.}}{31}{}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Original frame captured from the highway environment.}}{31}{}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Grayscale conversion and median filtering.}}{32}{}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Frame after the image preprocessing phase. Frame coordinates (Red). Trapezoid major base (Green). Trapezoid minor base (Orange). Trapezoid height (Yellow).}}{33}{}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Finding the~\gls {RP} nodes. First pixel row~\gls {ROI} (Red).~\gls {RP} nodes (Blue). }}{34}{}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Intensity level first row~\gls {ROI}.}}{34}{}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Illustration of the algorithm. First iteration (Fig.~\ref {fig:u1}). Second iteration (Fig.~\ref {fig:u2}). Green and Yellow colors represent~\gls {CP} node and~\gls {RP} node, respectively.}}{36}{}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces Hardware setup.}}{37}{}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces Results in different light scenarios. clear sky (a); under different tunnels with different luminosity (b)-(e); night road (f).}}{38}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Actor-Critic architecture: the actor produces an action given the current state of the environment, and the critic produces a temporal difference (TD) error signal given the state, the action, and the reward.}}{46}{}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces The intersection scenario for the pedestrian collision avoidance problem.}}{47}{}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces The agent-environment interaction for the pedestrian collision avoidance problem.}}{47}{}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces The vehicle model.}}{48}{}%
\contentsline {figure}{\numberline {5.5}{\ignorespaces Trajectory reward function term.}}{52}{}%
\contentsline {figure}{\numberline {5.6}{\ignorespaces Pedestrian reward function term.}}{53}{}%
\contentsline {figure}{\numberline {5.7}{\ignorespaces Combination of the trajectory and pedestrian reward function terms.}}{54}{}%
\contentsline {figure}{\numberline {5.8}{\ignorespaces Moving mean value and standard deviation of the total reward during the training process.}}{56}{}%
\contentsline {figure}{\numberline {5.9}{\ignorespaces Maximum trajectory tracking error distribution.}}{57}{}%
\contentsline {figure}{\numberline {5.10}{\ignorespaces Minimum vehicle-pedestrian distance distribution.}}{57}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces \gls {av} control architecture. The hacker corrupts the outputs of the sensory system.}}{60}{}%
\contentsline {figure}{\numberline {6.2}{\ignorespaces \gls {imps} architecture.}}{63}{}%
\contentsline {figure}{\numberline {6.3}{\ignorespaces Errors $e_{\zeta }$. Top-left: $e_x$. Top-right: $e_y$. Bottom-left: $e_{\psi }$. Bottom-right: $e_v$. }}{66}{}%
\contentsline {figure}{\numberline {6.4}{\ignorespaces Functional cost $J^\ast $. }}{67}{}%
\contentsline {figure}{\numberline {6.5}{\ignorespaces KL Divergences $D^{\mathrm {KL}}$. Top-left: $D^{\mathrm {KL}}\big (\bar {E}_x \| Q_x\big )$. Top-right: $D^{\mathrm {KL}}\big (\bar {E}_y \| Q_y\big )$. Bottom-left: $D^{\mathrm {KL}}\big (\bar {E}_\psi \| Q_\psi \big )$. Bottom-right: $D^{\mathrm {KL}}\big (\bar {E}_v \| Q_v\big )$. }}{67}{}%
\contentsline {figure}{\numberline {6.6}{\ignorespaces \gls {csvm} Confusion Matrix. }}{68}{}%
\contentsline {figure}{\numberline {6.7}{\ignorespaces No attacks.}}{70}{}%
\contentsline {figure}{\numberline {6.8}{\ignorespaces \gls {ra}.}}{71}{}%
\contentsline {figure}{\numberline {6.9}{\ignorespaces \gls {dos} attack.}}{71}{}%
\contentsline {figure}{\numberline {6.10}{\ignorespaces \gls {ra} mitigation using \gls {imps}. Start time attack and duration are $t_a= \SI {10}{\second }$ and $T_d=\SI {2}{\second }$, respectively. Top-left: Heading angle. Top-right: Velocity. Bottom: Position. }}{72}{}%
\contentsline {figure}{\numberline {6.11}{\ignorespaces Safe region and error trajectories. }}{72}{}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {B.1}{\ignorespaces Actor-Critic architecture: the actor produces an action given the current state of the environment, and the critic produces a temporal difference (TD) error signal given the state, the action, and the reward.}}{83}{}%
